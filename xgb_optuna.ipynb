{"cells": [{"cell_type": "markdown", "id": "17aa7163", "metadata": {}, "source": ["<p style=\"text-align: center\"><img src=\"https://gitlab.aicrowd.com/aicrowd/assets/-/raw/master/challenges/clock-decomposition/notebook-banner.jpg?inline=false\" alt=\"Drawing\" style=\"height: 400px;\"/></p>"]}, {"cell_type": "markdown", "id": "e94d0c9a", "metadata": {"heading_collapsed": "true", "tags": []}, "source": ["# What is the notebook about?\n", "\n", "The challenge is to use the features extracted from the Clock Drawing Test to build an automated and algorithm to predict whether each participant is one of three phases:\n", "\n", "1)    Pre-Alzheimer\u2019s (Early Warning)\n", "2)    Post-Alzheimer\u2019s (Detection)\n", "3)    Normal (Not an Alzheimer\u2019s patient)\n", "\n", "In machine learning terms: this is a 3-class classification task.\n", "\n", "# How to use this notebook? \ud83d\udcdd\n", "\n", "<p style=\"text-align: center\"><img src=\"https://gitlab.aicrowd.com/aicrowd/assets/-/raw/master/notebook/aicrowd_notebook_submission_flow.png?inline=false\" alt=\"notebook overview\" style=\"width: 650px;\"/></p>\n", "\n", "- **Update the config parameters**. You can define the common variables here\n", "\n", "Variable | Description\n", "--- | ---\n", "`AICROWD_DATASET_PATH` | Path to the file containing test data (The data will be available at `/ds_shared_drive/` on aridhia workspace). This should be an absolute path.\n", "`AICROWD_PREDICTIONS_PATH` | Path to write the output to.\n", "`AICROWD_ASSETS_DIR` | In case your notebook needs additional files (like model weights, etc.,), you can add them to a directory and specify the path to the directory here (please specify relative path). The contents of this directory will be sent to AIcrowd for evaluation.\n", "`AICROWD_API_KEY` | In order to submit your code to AIcrowd, you need to provide your account's API key. This key is available at https://www.aicrowd.com/participants/me\n", "\n", "- **Installing packages**. Please use the [Install packages \ud83d\uddc3](#install-packages-) section to install the packages\n", "- **Training your models**. All the code within the [Training phase \u2699\ufe0f](#training-phase-) section will be skipped during evaluation. **Please make sure to save your model weights in the assets directory and load them in the predictions phase section** "]}, {"cell_type": "markdown", "id": "40b92531", "metadata": {"tags": []}, "source": ["# Setup AIcrowd Utilities \ud83d\udee0\n", "\n", "We use this to bundle the files for submission and create a submission on AIcrowd. Do not edit this block."]}, {"cell_type": "code", "execution_count": 1, "id": "cd27ab03", "metadata": {}, "outputs": [], "source": ["!pip install -q -U aicrowd-cli"]}, {"cell_type": "code", "execution_count": 2, "id": "2461bb1a", "metadata": {}, "outputs": [], "source": ["%load_ext aicrowd.magic"]}, {"cell_type": "markdown", "id": "16a262fe", "metadata": {"tags": []}, "source": ["# AIcrowd Runtime Configuration \ud83e\uddf7\n", "\n", "Define configuration parameters. Please include any files needed for the notebook to run under `ASSETS_DIR`. We will copy the contents of this directory to your final submission file \ud83d\ude42\n", "\n", "The dataset is available under `/ds_shared_drive` on the workspace."]}, {"cell_type": "code", "execution_count": 3, "id": "f64720ba", "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "# Please use the absolute for the location of the dataset.\n", "# Or you can use relative path with `os.getcwd() + \"test_data/validation.csv\"`\n", "AICROWD_DATASET_PATH = os.getenv(\"DATASET_PATH\", \"/ds_shared_drive/validation.csv\")\n", "AICROWD_PREDICTIONS_PATH = os.getenv(\"PREDICTIONS_PATH\", \"predictions.csv\")\n", "AICROWD_ASSETS_DIR = \"assets\"\n"]}, {"cell_type": "markdown", "id": "c9f67a26", "metadata": {"tags": []}, "source": ["# Install packages \ud83d\uddc3\n", "\n", "Please add all pacakage installations in this section"]}, {"cell_type": "code", "execution_count": 4, "id": "418e3bf7", "metadata": {}, "outputs": [], "source": ["!pip install -q numpy pandas"]}, {"cell_type": "code", "execution_count": 5, "id": "3ac167a3", "metadata": {}, "outputs": [], "source": ["!pip install -q -U fastcore fastai"]}, {"cell_type": "code", "execution_count": 6, "id": "03d7d0e0", "metadata": {}, "outputs": [], "source": ["!pip install -q torch"]}, {"cell_type": "code", "execution_count": 7, "id": "67bb74ae", "metadata": {}, "outputs": [], "source": ["!pip install -q optuna"]}, {"cell_type": "code", "execution_count": 8, "id": "1889741f", "metadata": {}, "outputs": [], "source": ["!pip install -q seaborn lightgbm scikit-learn"]}, {"cell_type": "code", "execution_count": 9, "id": "1aeac65d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Requirement already satisfied: xgboost in ./conda/lib/python3.8/site-packages (1.4.1)\r\n", "Requirement already satisfied: scipy in ./conda/lib/python3.8/site-packages (from xgboost) (1.6.3)\r\n", "Requirement already satisfied: numpy in ./conda/lib/python3.8/site-packages (from xgboost) (1.19.5)\r\n"]}], "source": ["!pip install xgboost"]}, {"cell_type": "code", "execution_count": 45, "id": "4c737355", "metadata": {}, "outputs": [], "source": ["!pip install -q scipy"]}, {"cell_type": "markdown", "id": "d7b2abda", "metadata": {"tags": []}, "source": ["# Define preprocessing code \ud83d\udcbb\n", "\n", "The code that is common between the training and the prediction sections should be defined here. During evaluation, we completely skip the training section. Please make sure to add any common logic between the training and prediction sections here."]}, {"cell_type": "markdown", "id": "bd6549ad", "metadata": {}, "source": ["### Import common packages\n", "\n", "Please import packages that are common for training and prediction phases here."]}, {"cell_type": "code", "execution_count": 46, "id": "28fb8980", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn.metrics import log_loss\n", "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n", "from sklearn.neighbors import NearestCentroid, LocalOutlierFactor, KNeighborsClassifier\n", "from sklearn.preprocessing import PolynomialFeatures, RobustScaler, LabelEncoder\n", "from sklearn.ensemble import RandomForestClassifier\n", "from fastai.tabular.all import *\n", "from sklearn.utils import compute_class_weight\n", "import xgboost as xgb\n", "import torch\n", "import optuna\n", "import joblib\n", "from scipy.stats import mode"]}, {"cell_type": "code", "execution_count": 11, "id": "99a758e4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<class 'pandas.core.frame.DataFrame'>\n", "RangeIndex: 32777 entries, 0 to 32776\n", "Columns: 122 entries, row_id to diagnosis\n", "dtypes: float64(111), int64(8), object(3)\n", "memory usage: 30.5+ MB\n"]}], "source": ["train_data = pd.read_csv(os.getenv(\"DATASET_PATH\", \"/ds_shared_drive/train.csv\"))\n", "train_data.info()"]}, {"cell_type": "markdown", "id": "c62b93f3", "metadata": {"tags": []}, "source": ["# Training phase \u2699\ufe0f\n", "\n", "You can define your training code here. This sections will be skipped during evaluation."]}, {"cell_type": "code", "execution_count": 12, "id": "66928325", "metadata": {}, "outputs": [{"data": {"text/plain": ["normal            31208\n", "post_alzheimer     1149\n", "pre_alzheimer       420\n", "Name: diagnosis, dtype: int64"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["# train_data = train_data[train_data.diagnosis=='normal'].sample(6000).append(train_data[train_data.diagnosis!='normal'])\n", "train_data.diagnosis.value_counts()"]}, {"cell_type": "markdown", "id": "3a2d589b", "metadata": {}, "source": ["## Load training data"]}, {"cell_type": "code", "execution_count": 13, "id": "bbb7a20a", "metadata": {}, "outputs": [], "source": ["# class_wts = compute_class_weight(class_weight='balanced', \n", "#                                 classes=['normal','post_alzheimer', 'pre_alzheimer'],y=train_data.diagnosis)\n", "# class_wts = torch.from_numpy(class_wts)\n", "class_wts = torch.FloatTensor([0.3501,  9.5088, 26.0135])\n", "# class_wts"]}, {"cell_type": "code", "execution_count": 14, "id": "e31a92d0", "metadata": {}, "outputs": [], "source": ["#train_data = train_data[train_data.diagnosis.isin(['post_alzheimer','pre_alzheimer'])]\\\n", "# .append(train_data[train_data.diagnosis.isin(['normal'])].sample(6000))\n", "#train_data.info()"]}, {"cell_type": "code", "execution_count": 15, "id": "3b26ade6", "metadata": {}, "outputs": [{"data": {"text/plain": ["\"interaction_generator = PolynomialFeatures(degree=2, interaction_only=True)\\ntrain_interactions = interaction_generator.fit_transform(train_data.fillna(0).drop(columns=['row_id', \\n                                                                                  'intersection_pos_rel_centre', \\n                                                                                  'diagnosis',\\n                                                                                  'actual_hour_digit',\\n                                                                                  'actual_minute_digit']))\\ntrain_interactions = pd.DataFrame(train_interactions)\\ntrain_interactions.columns = interaction_generator.get_feature_names()\\ntrain_interactions.info()\\ntrain_data = pd.concat([train_data, train_interactions], axis=1)\\ntrain_data.info()\""]}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": ["'''interaction_generator = PolynomialFeatures(degree=2, interaction_only=True)\n", "train_interactions = interaction_generator.fit_transform(train_data.fillna(0).drop(columns=['row_id', \n", "                                                                                  'intersection_pos_rel_centre', \n", "                                                                                  'diagnosis',\n", "                                                                                  'actual_hour_digit',\n", "                                                                                  'actual_minute_digit']))\n", "train_interactions = pd.DataFrame(train_interactions)\n", "train_interactions.columns = interaction_generator.get_feature_names()\n", "train_interactions.info()\n", "train_data = pd.concat([train_data, train_interactions], axis=1)\n", "train_data.info()'''"]}, {"cell_type": "code", "execution_count": 16, "id": "4ebe7c91", "metadata": {}, "outputs": [{"data": {"text/plain": ["((33139, 123), (32777, 122))"]}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": ["dep_var = 'diagnosis'\n", "cont_var = train_data.drop(columns=['row_id', 'intersection_pos_rel_centre', \n", "                                    'diagnosis',\n", "                                    'actual_hour_digit',\n", "                                    'actual_minute_digit'\n", "                                   ]).columns.tolist() \n", "cat_var = ['intersection_pos_rel_centre']\n", "\n", "procs = [FillMissing, Categorify, Normalize]\n", "\n", "kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n", "\n", "val_data = pd.read_csv(os.getenv(\"DATASET_PATH\", \"/ds_shared_drive/validation.csv\"))\n", "val_actuals = pd.read_csv(os.getenv(\"DATASET_PATH\", \"/ds_shared_drive/validation_ground_truth.csv\"))\n", "val_merge = val_data.merge(val_actuals, on=['row_id'])\n", "train_data_full = train_data.append(val_merge[train_data.columns])\n", "train_data_full['iprc'] = train_data_full['intersection_pos_rel_centre']\n", "\n", "iprc_num = {'iprc':{'TL':4, 'BL':3, 'TR':2, 'BR':1}}\n", "train_data_full.replace(iprc_num, inplace=True)\n", "train_data.replace(iprc_num, inplace=True)\n", "val_merge['iprc'] = val_merge['intersection_pos_rel_centre']\n", "val_merge.replace(iprc_num, inplace=True)\n", "\n", "target_col = \"diagnosis\"\n", "target_values = [\"normal\", \"post_alzheimer\", \"pre_alzheimer\"]\n", "seed=2021\n", "'''df_pos = train_data_full[train_data_full[target_col].isin(target_values[1:])]\n", "nb_pos = df_pos.shape[0]\n", "nb_neg = nb_pos\n", "df_neg = train_data_full[train_data_full[target_col] == \"normal\"].sample(n=nb_neg, random_state=seed)\n", "df_samples = pd.concat([df_pos, df_neg]).sample(frac=1).reset_index(drop=True)\n", "train_data_full = df_samples.copy(deep=True)'''\n", "\n", "\n", "\n", "train_data_full.shape, train_data.shape"]}, {"cell_type": "code", "execution_count": 17, "id": "aa7f7a18", "metadata": {}, "outputs": [{"data": {"text/plain": ["TL    10362\n", "BL     5283\n", "TR     4961\n", "BR     2467\n", "Name: intersection_pos_rel_centre, dtype: int64"]}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": ["train_data_full['intersection_pos_rel_centre'].value_counts()"]}, {"cell_type": "markdown", "id": "b648926e", "metadata": {}, "source": ["## Train your model"]}, {"cell_type": "code", "execution_count": 18, "id": "8f6eb1cb", "metadata": {}, "outputs": [{"data": {"text/plain": ["(33139, 124)"]}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": ["nc = NearestCentroid()\n", "rs = RobustScaler()\n", "knn = KNeighborsClassifier(n_neighbors=5)\n", "\n", "nc.fit(train_data_full.drop(columns=['row_id',\n", "                                     'intersection_pos_rel_centre',\n", "                                     'diagnosis']).fillna(-1), train_data_full['diagnosis'])\n", "train_data_full['nc'] = nc.predict(train_data_full.drop(columns=['row_id',\n", "                                     'intersection_pos_rel_centre',\n", "                                     'diagnosis']).fillna(-1)).reshape((-1,1))\n", "\n", "\n", "nc_num = {'nc':{'normal':1, 'pre_alzheimer':2, 'post_alzheimer':3}, \n", "          'knn':{'normal':1, 'pre_alzheimer':2, 'post_alzheimer':3}}\n", "\n", "train_data_full.replace(nc_num, inplace=True)\n", "\n", "train_data_full.shape"]}, {"cell_type": "code", "execution_count": 19, "id": "51cea809", "metadata": {}, "outputs": [{"data": {"text/plain": ["(33139, 7382)"]}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": ["poly_feats = PolynomialFeatures(degree=2, interaction_only=True)\n", "X_train_int = poly_feats.fit_transform(train_data_full.drop(columns=['row_id',\n", "                                                                     'intersection_pos_rel_centre',\n", "                                                                     'diagnosis']).fillna(-1))\n", "X_train_int.shape"]}, {"cell_type": "code", "execution_count": 20, "id": "7edf7bb2", "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0.00000000e+00, 2.51335634e-04, 6.37351371e-05, ...,\n", "       2.29101112e-05, 3.03049883e-05, 4.16668412e-05])"]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": ["rf_poly = RandomForestClassifier(n_estimators=200)\n", "rf_poly.fit(X_train_int, train_data_full['diagnosis'])\n", "importances = rf_poly.feature_importances_\n", "importances"]}, {"cell_type": "code", "execution_count": 21, "id": "925f1af6", "metadata": {}, "outputs": [{"data": {"text/plain": ["(33139, 500)"]}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": ["int_index = (-importances).argsort()[:500]\n", "X_poly = X_train_int[:,int_index]\n", "df_poly = pd.DataFrame(X_poly)\n", "df_poly.columns = [poly_feats.get_feature_names()[i] for i in int_index]\n", "df_poly.shape"]}, {"cell_type": "code", "execution_count": 22, "id": "cff561ca", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<class 'pandas.core.frame.DataFrame'>\n", "RangeIndex: 33139 entries, 0 to 33138\n", "Columns: 624 entries, row_id to x75 x90\n", "dtypes: float64(612), int64(9), object(3)\n", "memory usage: 157.8+ MB\n"]}], "source": ["train_data_full = pd.concat([train_data_full.reset_index(drop=True), df_poly.reset_index(drop=True)], axis=1)\n", "train_data_full.info()"]}, {"cell_type": "code", "execution_count": 23, "id": "afa3c315", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(26511, 624) (6628, 624)\n"]}, {"data": {"text/plain": ["(normal            25210\n", " post_alzheimer      958\n", " pre_alzheimer       343\n", " Name: diagnosis, dtype: int64,\n", " normal            6268\n", " post_alzheimer     253\n", " pre_alzheimer      107\n", " Name: diagnosis, dtype: int64)"]}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": ["train_data_all_in = train_data_full.copy(deep=True)\n", "train_data_full, val_merge =  train_test_split(train_data_all_in, test_size=0.2, random_state=42)\n", "print(train_data_full.shape, val_merge.shape)\n", "train_data_full.diagnosis.value_counts(), val_merge.diagnosis.value_counts()"]}, {"cell_type": "code", "execution_count": 24, "id": "a622cbc2", "metadata": {}, "outputs": [{"data": {"text/plain": ["((26511, 624), (6628, 624))"]}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": ["train_data_full = train_data_full.fillna(-1)\n", "val_merge = val_merge.fillna(-1)\n", "train_data_full.shape, val_merge.shape"]}, {"cell_type": "code", "execution_count": 25, "id": "dd31e466", "metadata": {}, "outputs": [], "source": ["le = LabelEncoder()\n", "le.fit(train_data_full.diagnosis)\n", "\n", "y_train = le.transform(train_data_full.diagnosis)\n", "y_test = le.transform(val_merge.diagnosis)\n", "dtrain = xgb.DMatrix(train_data_full.drop(columns=['row_id','intersection_pos_rel_centre','diagnosis']),\n", "                                                   y_train)\n", "dtest = xgb.DMatrix(val_merge.drop(columns=['row_id','intersection_pos_rel_centre','diagnosis']),\n", "                                            y_test)\n"]}, {"cell_type": "code", "execution_count": null, "id": "4ee364f1", "metadata": {}, "outputs": [], "source": ["\n", "def objective(trial: optuna.Trial):\n", "    param = {\n", "        \"num_class\":3,\n", "        \"silent\":1, \n", "        \"objective\":\"multi:softmax\",\n", "        \"eval_metric\":\"mlogloss\",\n", "        \"booster\":trial.suggest_categorical(\"booster\",['gbtree','gblinear','dart']),\n", "        \"lambda\":trial.suggest_loguniform(\"lambda\", 1e-8, 1.0), \n", "        \"alpha\":trial.suggest_loguniform(\"alpha\", 1e-8, 1.0)\n", "    }\n", "\n", "    if((param['booster']==\"gbtree\") or (param['booster']=='gblinear')):\n", "        param['max_depth'] = trial.suggest_int('max_depth', 1, 9)\n", "        param['eta'] = trial.suggest_loguniform('eta', 1e-8, 1.0)\n", "        param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n", "        param['grow_policy'] = trial.suggest_categorical('grow_policy',['depthwise','lossguide'])\n", "        \n", "    if(param['booster']=='dart'):\n", "        param['sample_type'] = trial.suggest_categorical('sample_type',['uniform','weighted'])\n", "        param['normalize_type'] = trial.suggest_categorical('normalize_type',['tree','forest'])\n", "        param['rate_drop'] = trial.suggest_loguniform('rate_drop',1e-8, 1.0)\n", "        param['skip_drop'] = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)\n", "\n", "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-mlogloss\")\n", "    bst = xgb.train(param, dtrain, evals=[(dtest, \"validation\")], callbacks=[pruning_callback])\n", "    preds = bst.predict(dtest)\n", "    \n", "    pred_1 = []\n", "    for pred in preds:\n", "        if(pred==0):\n", "            pred = [0.6, 0.2, 0.2]\n", "        elif(pred==1):\n", "            pred = [0.2, 0.6, 0.2]\n", "        else:\n", "            pred = [0.2, 0.2, 0.6]\n", "        pred_1.append(pred)\n", "    preds = np.array(pred_1)\n", "    # preds.shape\n", "    \n", "    per_ = log_loss(val_merge['diagnosis'].values, preds)\n", "    return per_\n", "\n", "study = optuna.create_study()\n", "study.optimize(objective)\n", "best_trial = study.best_trial\n"]}, {"cell_type": "code", "execution_count": null, "id": "af89481e", "metadata": {}, "outputs": [], "source": ["# best_trial"]}, {"cell_type": "code", "execution_count": null, "id": "ede65b9e", "metadata": {}, "outputs": [], "source": ["#  {'n_layers': 4, 'n_epochs': 3, 'batch_size': 32, 'num_units_layer_0': 1000, 'dropout_p_layer_0': 0.9, 'num_units_layer_1': 1100, 'dropout_p_layer_1': 0.35000000000000003, 'num_units_layer_2': 800, 'dropout_p_layer_2': 0.6000000000000001, 'emb_drop': 0.5}. \n"]}, {"cell_type": "code", "execution_count": null, "id": "ccd1a357", "metadata": {}, "outputs": [], "source": ["#  Best is trial 9 with value: 0.5676789754577772.\n", "best_parameters = {'booster': 'dart', \n", "                   'lambda': 0.6718751470224827, \n", "                   'alpha': 0.01667738524563801, \n", "                   'sample_type': 'uniform', \n", "                   'normalize_type': 'tree', \n", "                   'rate_drop': 0.003246725607740222, \n", "                   'skip_drop': 6.0236365624270305e-06}"]}, {"cell_type": "code", "execution_count": 26, "id": "16d32d77", "metadata": {}, "outputs": [], "source": ["param = {\n", "        \"num_class\":3,\n", "        \"silent\":1, \n", "        \"objective\":\"multi:softmax\",\n", "        \"eval_metric\":\"mlogloss\",\n", "        'booster': 'dart', \n", "    'lambda': 0.6718751470224827, \n", "    'alpha': 0.01667738524563801, \n", "    'sample_type': 'uniform', \n", "    'normalize_type': 'tree', \n", "    'rate_drop': 0.003246725607740222, \n", "    'skip_drop': 6.0236365624270305e-06\n", "    }\n", "\n", "#y_train1 = le.transform(train_data_all_in.diagnosis)\n", "#dtrain1 = xgb.DMatrix(train_data_all_in.drop(columns=['row_id','intersection_pos_rel_centre','diagnosis']),\n", "#                                                   y_train1)\n", "\n", "#bst = xgb.train(param, dtrain1\n", "#                , evals=[(dtest, \"validation\")]\n", "#               )\n", "    "]}, {"cell_type": "code", "execution_count": 30, "id": "97c87efd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["----------------------------------------\n", "Running for fold 0\n", "[04:57:38] WARNING: ../src/learner.cc:573: \n", "Parameters: { \"silent\" } might not be used.\n", "\n", "  This may not be accurate due to some parameters are only used in language bindings but\n", "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n", "  verification. Please open an issue if you find above cases.\n", "\n", "\n", "[0]\tvalidation-mlogloss:0.75256\n", "[1]\tvalidation-mlogloss:0.55988\n", "[2]\tvalidation-mlogloss:0.43791\n", "[3]\tvalidation-mlogloss:0.35738\n", "[4]\tvalidation-mlogloss:0.30283\n", "[5]\tvalidation-mlogloss:0.26516\n", "[6]\tvalidation-mlogloss:0.23956\n", "[7]\tvalidation-mlogloss:0.22189\n", "[8]\tvalidation-mlogloss:0.21000\n", "[9]\tvalidation-mlogloss:0.20150\n", "----------------------------------------\n", "Running for fold 1\n", "[04:58:03] WARNING: ../src/learner.cc:573: \n", "Parameters: { \"silent\" } might not be used.\n", "\n", "  This may not be accurate due to some parameters are only used in language bindings but\n", "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n", "  verification. Please open an issue if you find above cases.\n", "\n", "\n", "[0]\tvalidation-mlogloss:0.75065\n", "[1]\tvalidation-mlogloss:0.55601\n", "[2]\tvalidation-mlogloss:0.43246\n", "[3]\tvalidation-mlogloss:0.35100\n", "[4]\tvalidation-mlogloss:0.29607\n", "[5]\tvalidation-mlogloss:0.25821\n", "[6]\tvalidation-mlogloss:0.23203\n", "[7]\tvalidation-mlogloss:0.21406\n", "[8]\tvalidation-mlogloss:0.20138\n", "[9]\tvalidation-mlogloss:0.19262\n", "----------------------------------------\n", "Running for fold 2\n", "[04:58:27] WARNING: ../src/learner.cc:573: \n", "Parameters: { \"silent\" } might not be used.\n", "\n", "  This may not be accurate due to some parameters are only used in language bindings but\n", "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n", "  verification. Please open an issue if you find above cases.\n", "\n", "\n", "[0]\tvalidation-mlogloss:0.74929\n", "[1]\tvalidation-mlogloss:0.55247\n", "[2]\tvalidation-mlogloss:0.42789\n", "[3]\tvalidation-mlogloss:0.34428\n", "[4]\tvalidation-mlogloss:0.28790\n", "[5]\tvalidation-mlogloss:0.24852\n", "[6]\tvalidation-mlogloss:0.22091\n", "[7]\tvalidation-mlogloss:0.20195\n", "[8]\tvalidation-mlogloss:0.18844\n", "[9]\tvalidation-mlogloss:0.17901\n", "----------------------------------------\n", "Running for fold 3\n", "[04:58:51] WARNING: ../src/learner.cc:573: \n", "Parameters: { \"silent\" } might not be used.\n", "\n", "  This may not be accurate due to some parameters are only used in language bindings but\n", "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n", "  verification. Please open an issue if you find above cases.\n", "\n", "\n", "[0]\tvalidation-mlogloss:0.75196\n", "[1]\tvalidation-mlogloss:0.55779\n", "[2]\tvalidation-mlogloss:0.43553\n", "[3]\tvalidation-mlogloss:0.35411\n", "[4]\tvalidation-mlogloss:0.29926\n", "[5]\tvalidation-mlogloss:0.26198\n", "[6]\tvalidation-mlogloss:0.23615\n", "[7]\tvalidation-mlogloss:0.21801\n", "[8]\tvalidation-mlogloss:0.20531\n", "[9]\tvalidation-mlogloss:0.19691\n", "----------------------------------------\n", "Running for fold 4\n", "[04:59:15] WARNING: ../src/learner.cc:573: \n", "Parameters: { \"silent\" } might not be used.\n", "\n", "  This may not be accurate due to some parameters are only used in language bindings but\n", "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n", "  verification. Please open an issue if you find above cases.\n", "\n", "\n", "[0]\tvalidation-mlogloss:0.75123\n", "[1]\tvalidation-mlogloss:0.55717\n", "[2]\tvalidation-mlogloss:0.43462\n", "[3]\tvalidation-mlogloss:0.35317\n", "[4]\tvalidation-mlogloss:0.29867\n", "[5]\tvalidation-mlogloss:0.26074\n", "[6]\tvalidation-mlogloss:0.23457\n", "[7]\tvalidation-mlogloss:0.21623\n", "[8]\tvalidation-mlogloss:0.20353\n", "[9]\tvalidation-mlogloss:0.19521\n"]}, {"data": {"text/plain": ["5"]}, "execution_count": 30, "metadata": {}, "output_type": "execute_result"}], "source": ["skf = StratifiedKFold(n_splits=5, random_state=2021, shuffle=True)\n", "\n", "le = LabelEncoder()\n", "le.fit(train_data_full.diagnosis)\n", "\n", "clfs = []\n", "for fold, (itrain, ivalid) in enumerate(skf.split(train_data_full, train_data_full.diagnosis)):\n", "    print(\"-\"*40)\n", "    print(f\"Running for fold {fold}\")\n", "    \n", "    y_train = le.transform(train_data_all_in.iloc[itrain,:].diagnosis)\n", "    y_test = le.transform(train_data_all_in.iloc[ivalid,:].diagnosis)\n", "    dtrain = xgb.DMatrix(train_data_all_in.iloc[itrain,:].drop(columns=['row_id','intersection_pos_rel_centre','diagnosis']),\n", "                                                       y_train)\n", "    dtest = xgb.DMatrix(train_data_all_in.iloc[ivalid,:].drop(columns=['row_id','intersection_pos_rel_centre','diagnosis']),\n", "                                                y_test)\n", "    \n", "    bst = xgb.train(param, dtrain, evals=[(dtest, \"validation\")])\n", "    clfs.append(bst)\n", "\n", "len(clfs)"]}, {"cell_type": "markdown", "id": "fb3a6c55", "metadata": {}, "source": ["## Save your trained model"]}, {"cell_type": "code", "execution_count": 31, "id": "cda9c82f", "metadata": {}, "outputs": [{"data": {"text/plain": ["['assets/model_lgb_meta.pkl']"]}, "execution_count": 31, "metadata": {}, "output_type": "execute_result"}], "source": ["i=0\n", "for clf in clfs:\n", "    joblib.dump(bst,f'{AICROWD_ASSETS_DIR}/model_'+str(i)+'.pkl')\n", "    i=i+1\n", "\n", "joblib.dump(le, f'{AICROWD_ASSETS_DIR}/le.pkl')\n", "\n", "poly_filename = f'{AICROWD_ASSETS_DIR}/poly_feats.pkl'\n", "joblib.dump(poly_feats, poly_filename)\n", "\n", "nc_filename = f'{AICROWD_ASSETS_DIR}/nc_model.pkl'\n", "joblib.dump(nc, nc_filename)\n", "\n", "nc_num_filename  = f'{AICROWD_ASSETS_DIR}/nc_num.pkl'\n", "joblib.dump(nc_num, nc_num_filename)\n", "\n", "rs_filename = f'{AICROWD_ASSETS_DIR}/rs.pkl'\n", "joblib.dump(rs, rs_filename)\n", "\n", "iprc_filename = f'{AICROWD_ASSETS_DIR}/iprc.pkl'\n", "joblib.dump(iprc_num, iprc_filename)\n", "\n", "meta = {\n", "    \"inter_index\": int_index\n", "}\n", "meta_filename = f'{AICROWD_ASSETS_DIR}/model_lgb_meta.pkl'\n", "joblib.dump(meta, meta_filename)"]}, {"cell_type": "markdown", "id": "0748c7bf", "metadata": {"tags": []}, "source": ["# Prediction phase \ud83d\udd0e\n", "\n", "Please make sure to save the weights from the training section in your assets directory and load them in this section"]}, {"cell_type": "code", "execution_count": 32, "id": "30a89554", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[<xgboost.core.Booster object at 0x7f9fbbaabb50>, <xgboost.core.Booster object at 0x7f9fbbaab910>, <xgboost.core.Booster object at 0x7f9fbbaab3a0>, <xgboost.core.Booster object at 0x7f9fbbaabdc0>, <xgboost.core.Booster object at 0x7f9fbbaabc70>]\n", "dict_keys(['inter_index'])\n"]}], "source": ["# model = load_model_from_assets_dir(AIAICROWD_ASSETS_DIR)\n", "# learners = [load_learner(AICROWD_ASSETS_DIR+'/fastai_tabular'+str(i)+'.mdl') for i in range(5)]\n", "# len(learners)\n", "clfs = []\n", "n = 5\n", "\n", "for i in range(n):\n", "    clfs.append(joblib.load(f'{AICROWD_ASSETS_DIR}/model_'+str(i)+'.pkl'))\n", "print(clfs)\n", "# bst = joblib.load(AICROWD_ASSETS_DIR+'/model.pkl')\n", "le = joblib.load(AICROWD_ASSETS_DIR+'/le.pkl')\n", "\n", "meta_filename = f'{AICROWD_ASSETS_DIR}/model_lgb_meta.pkl'\n", "meta = joblib.load(meta_filename)\n", "print(meta.keys())\n", "\n", "poly_filename = f'{AICROWD_ASSETS_DIR}/poly_feats.pkl'\n", "poly_feats = joblib.load(poly_filename)\n", "\n", "nc_filename = f'{AICROWD_ASSETS_DIR}/nc_model.pkl'\n", "nc = joblib.load(nc_filename)\n", "\n", "nc_num_filename  = f'{AICROWD_ASSETS_DIR}/nc_num.pkl'\n", "nc_num = joblib.load(nc_num_filename)\n", "\n", "rs_filename = f'{AICROWD_ASSETS_DIR}/rs.pkl'\n", "rs = joblib.load(rs_filename)\n", "\n", "iprc_filename = f'{AICROWD_ASSETS_DIR}/iprc.pkl'\n", "iprc_num = joblib.load(iprc_filename)\n", "\n", "\n", "int_index = meta['inter_index']"]}, {"cell_type": "code", "execution_count": 33, "id": "55217c9c", "metadata": {}, "outputs": [{"data": {"text/plain": ["5"]}, "execution_count": 33, "metadata": {}, "output_type": "execute_result"}], "source": ["len(clfs)"]}, {"cell_type": "markdown", "id": "810873bd", "metadata": {}, "source": ["## Load test data"]}, {"cell_type": "code", "execution_count": 34, "id": "b933d60b", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>row_id</th>\n", "      <th>number_of_digits</th>\n", "      <th>missing_digit_1</th>\n", "      <th>missing_digit_2</th>\n", "      <th>missing_digit_3</th>\n", "      <th>missing_digit_4</th>\n", "      <th>missing_digit_5</th>\n", "      <th>missing_digit_6</th>\n", "      <th>missing_digit_7</th>\n", "      <th>missing_digit_8</th>\n", "      <th>...</th>\n", "      <th>top_area_perc</th>\n", "      <th>bottom_area_perc</th>\n", "      <th>left_area_perc</th>\n", "      <th>right_area_perc</th>\n", "      <th>hor_count</th>\n", "      <th>vert_count</th>\n", "      <th>eleven_ten_error</th>\n", "      <th>other_error</th>\n", "      <th>time_diff</th>\n", "      <th>centre_dot_detect</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>LA9JQ1JZMJ9D2MBZV</td>\n", "      <td>11.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>0.500272</td>\n", "      <td>0.499368</td>\n", "      <td>0.553194</td>\n", "      <td>0.446447</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>1</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>PSSRCWAPTAG72A1NT</td>\n", "      <td>6.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>0.572472</td>\n", "      <td>0.427196</td>\n", "      <td>0.496352</td>\n", "      <td>0.503273</td>\n", "      <td>0</td>\n", "      <td>1</td>\n", "      <td>0</td>\n", "      <td>1</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>GCTODIZJB42VCBZRZ</td>\n", "      <td>11.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>0.494076</td>\n", "      <td>0.505583</td>\n", "      <td>0.503047</td>\n", "      <td>0.496615</td>\n", "      <td>1</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>7YMVQGV1CDB1WZFNE</td>\n", "      <td>3.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>...</td>\n", "      <td>0.555033</td>\n", "      <td>0.444633</td>\n", "      <td>0.580023</td>\n", "      <td>0.419575</td>\n", "      <td>0</td>\n", "      <td>1</td>\n", "      <td>0</td>\n", "      <td>1</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>PHEQC6DV3LTFJYIJU</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>0.603666</td>\n", "      <td>0.395976</td>\n", "      <td>0.494990</td>\n", "      <td>0.504604</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>1</td>\n", "      <td>150.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>5 rows \u00d7 121 columns</p>\n", "</div>"], "text/plain": ["              row_id  number_of_digits  missing_digit_1  missing_digit_2  \\\n", "0  LA9JQ1JZMJ9D2MBZV              11.0              0.0              0.0   \n", "1  PSSRCWAPTAG72A1NT               6.0              1.0              1.0   \n", "2  GCTODIZJB42VCBZRZ              11.0              0.0              0.0   \n", "3  7YMVQGV1CDB1WZFNE               3.0              1.0              0.0   \n", "4  PHEQC6DV3LTFJYIJU               1.0              1.0              1.0   \n", "\n", "   missing_digit_3  missing_digit_4  missing_digit_5  missing_digit_6  \\\n", "0              0.0              0.0              0.0              0.0   \n", "1              0.0              1.0              1.0              0.0   \n", "2              0.0              0.0              1.0              0.0   \n", "3              1.0              0.0              1.0              1.0   \n", "4              1.0              1.0              1.0              1.0   \n", "\n", "   missing_digit_7  missing_digit_8  ...  top_area_perc  bottom_area_perc  \\\n", "0              0.0              0.0  ...       0.500272          0.499368   \n", "1              0.0              0.0  ...       0.572472          0.427196   \n", "2              0.0              0.0  ...       0.494076          0.505583   \n", "3              1.0              1.0  ...       0.555033          0.444633   \n", "4              1.0              0.0  ...       0.603666          0.395976   \n", "\n", "   left_area_perc  right_area_perc  hor_count  vert_count  eleven_ten_error  \\\n", "0        0.553194         0.446447          0           0                 0   \n", "1        0.496352         0.503273          0           1                 0   \n", "2        0.503047         0.496615          1           0                 0   \n", "3        0.580023         0.419575          0           1                 0   \n", "4        0.494990         0.504604          0           0                 0   \n", "\n", "   other_error  time_diff  centre_dot_detect  \n", "0            1        NaN                NaN  \n", "1            1        NaN                NaN  \n", "2            0        0.0                0.0  \n", "3            1        NaN                NaN  \n", "4            1      150.0                0.0  \n", "\n", "[5 rows x 121 columns]"]}, "execution_count": 34, "metadata": {}, "output_type": "execute_result"}], "source": ["test_data = pd.read_csv(AICROWD_DATASET_PATH)\n", "test_data.head()"]}, {"cell_type": "markdown", "id": "efc7c7e0", "metadata": {}, "source": ["## Generate predictions"]}, {"cell_type": "code", "execution_count": 35, "id": "7241ddad", "metadata": {}, "outputs": [{"data": {"text/plain": ["(362, 623)"]}, "execution_count": 35, "metadata": {}, "output_type": "execute_result"}], "source": ["'''test_rs = rs.fit_transform(test_data.drop(columns=['row_id',\n", "                                                  'intersection_pos_rel_centre']).fillna(-1))\n", "test_data['nc'] = nc.predict(test_rs)'''\n", "\n", "test_data['iprc'] = test_data['intersection_pos_rel_centre']\n", "test_data.replace(iprc_num, inplace=True)\n", "\n", "test_data['nc'] = nc.predict(test_data.drop(columns=['row_id',\n", "                                     'intersection_pos_rel_centre']).fillna(-1)).reshape((-1,1))\n", "\n", "test_data.replace(nc_num, inplace=True)\n", "\n", "\n", "X_test_int = poly_feats.transform(test_data.drop(columns=['row_id',\n", "                                                         'intersection_pos_rel_centre']).fillna(-1))\n", "X_poly_test = X_test_int[:,int_index]\n", "df_poly_test = pd.DataFrame(X_poly_test)\n", "df_poly_test.columns = [poly_feats.get_feature_names()[i] for i in int_index]\n", "test_data = pd.concat([test_data, df_poly_test], axis=1)\n", "test_data.shape"]}, {"cell_type": "code", "execution_count": 68, "id": "4690ed2a", "metadata": {}, "outputs": [], "source": ["dsub = xgb.DMatrix(test_data.drop(columns=['row_id','intersection_pos_rel_centre']))\n", "\n", "prs = []\n", "for clf in clfs:\n", "    prs.append(clf.predict(dsub))\n", "prs = np.array(prs).T\n", "# preds = mode(prs, axis=1)[0]\n", "preds = prs.mean(axis=1)"]}, {"cell_type": "code", "execution_count": 69, "id": "49a6f668", "metadata": {}, "outputs": [{"data": {"text/plain": ["(362, 3)"]}, "execution_count": 69, "metadata": {}, "output_type": "execute_result"}], "source": ["pred_1 = []\n", "for pred in list(preds):\n", "    if(pred<0.5):\n", "        pred = [0.6, 0.2, 0.2]\n", "    elif(pred<1.5):\n", "        pred = [0.2, 0.6, 0.2]\n", "    else:\n", "        pred = [0.2, 0.2, 0.6]\n", "    pred_1.append(pred)\n", "preds = np.array(pred_1)\n", "preds.shape"]}, {"cell_type": "code", "execution_count": 70, "id": "97f2f894", "metadata": {}, "outputs": [], "source": ["predictions = {\n", "    \"row_id\": test_data[\"row_id\"].values,\n", "    \"normal_diagnosis_probability\": preds[:,0],\n", "    \"post_alzheimer_diagnosis_probability\": preds[:,1],\n", "    \"pre_alzheimer_diagnosis_probability\": preds[:,2],\n", "}\n", "\n", "predictions_df = pd.DataFrame.from_dict(predictions)"]}, {"cell_type": "markdown", "id": "06dd0eeb", "metadata": {}, "source": ["## Save predictions \ud83d\udce8"]}, {"cell_type": "code", "execution_count": 71, "id": "b38cfbb4", "metadata": {}, "outputs": [], "source": ["predictions_df.to_csv(AICROWD_PREDICTIONS_PATH, index=False)"]}, {"cell_type": "markdown", "id": "a16a8a7b", "metadata": {"tags": []}, "source": ["# Submit to AIcrowd \ud83d\ude80\n", "\n", "**NOTE: PLEASE SAVE THE NOTEBOOK BEFORE SUBMITTING IT (Ctrl + S)**"]}, {"cell_type": "code", "execution_count": 67, "id": "1f2c9648", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[32mAPI Key valid\u001b[0m\n", "\u001b[32mSaved API Key successfully!\u001b[0m\n", "Using notebook: /home/desktop0/python-optuna-xgboost.ipynb for submission...\n", "Removing existing files from submission directory...\n", "Scrubbing API keys from the notebook...\n", "Collecting notebook...\n", "Validating the submission...\n", "Executing install.ipynb...\n", "[NbConvertApp] Converting notebook /home/desktop0/submission/install.ipynb to notebook\n", "[NbConvertApp] Executing notebook with kernel: python\n", "[NbConvertApp] Writing 4234 bytes to /home/desktop0/submission/install.nbconvert.ipynb\n", "Executing predict.ipynb...\n", "[NbConvertApp] Converting notebook /home/desktop0/submission/predict.ipynb to notebook\n", "[NbConvertApp] Executing notebook with kernel: python\n", "[NbConvertApp] Writing 24283 bytes to /home/desktop0/submission/predict.nbconvert.ipynb\n", "\u001b[2K\u001b[1;34msubmission.zip\u001b[0m \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[35m100.0%\u001b[0m \u2022 \u001b[32m76.0/76.0 MB\u001b[0m \u2022 \u001b[31m2.4 MB/s\u001b[0m \u2022 \u001b[36m0:00:00\u001b[0m[0m \u2022 \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n", "\u001b[?25h                                                 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                                                 \n", "                                                 \u2502 \u001b[1mSuccessfully submitted!\u001b[0m \u2502                                                 \n", "                                                 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                 \n", "\u001b[3m                                                       Important links                                                       \u001b[0m\n", "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n", "\u2502  This submission \u2502 https://www.aicrowd.com/challenges/addi-alzheimers-detection-challenge/submissions/144539              \u2502\n", "\u2502                  \u2502                                                                                                        \u2502\n", "\u2502  All submissions \u2502 https://www.aicrowd.com/challenges/addi-alzheimers-detection-challenge/submissions?my_submissions=true \u2502\n", "\u2502                  \u2502                                                                                                        \u2502\n", "\u2502      Leaderboard \u2502 https://www.aicrowd.com/challenges/addi-alzheimers-detection-challenge/leaderboards                    \u2502\n", "\u2502                  \u2502                                                                                                        \u2502\n", "\u2502 Discussion forum \u2502 https://discourse.aicrowd.com/c/addi-alzheimers-detection-challenge                                    \u2502\n", "\u2502                  \u2502                                                                                                        \u2502\n", "\u2502   Challenge page \u2502 https://www.aicrowd.com/challenges/addi-alzheimers-detection-challenge                                 \u2502\n", "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"]}], "source": ["!DATASET_PATH=$AICROWD_DATASET_PATH \\\n", "aicrowd notebook submit \\\n", "    --assets-dir $AICROWD_ASSETS_DIR \\\n", "    --challenge addi-alzheimers-detection-challenge"]}, {"cell_type": "code", "execution_count": null, "id": "d1844bae", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "toc-autonumbering": false, "toc-showcode": false, "toc-showmarkdowntxt": false, "toc-showtags": false}, "nbformat": 4, "nbformat_minor": 5}